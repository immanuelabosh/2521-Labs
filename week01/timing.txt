 |Input	Initial	Has	Number	AvgTime	AvgTime
Size	Order	Dups	of runs	forusel	forsort
5000	random	no	5	00130	00004
5000	sorted	no	5	00054	00003
5000	reverse	no	5	00049	00003
5000	random	yes	5	00130	00003
5000	sorted	yes	5	00051	00001
5000	reverse	yes	5	00053	00002
10000	random	no	5	00570	00007
10000	sorted	no	5	00220	00003
10000	reverse	no	5	00220	00004
10000	random	yes	5	00620	00007
10000	sorted	yes	5	00200	00003
10000	reverse	yes	5	00210	00005
20000	random	no	5	03000	00011
20000	sorted	no	5	00890	00008
20000	reverse	no	5	00860	00009
20000	random	yes	5	03100	00015
20000	sorted	yes	5	00850	00008
20000	reverse	yes	5	00880	00009
30000	random	no	5	11000	00022
30000	sorted	no	5	02100	00011
30000	reverse	no	5	02200	00013
30000	random	yes	5	09700	00023
30000	sorted	yes	5	01900	00015
30000	reverse	yes	5	02100	00012

Explanation
------------------------------
NOTE times are in milliseconds
Even though the input only doubles from 5000 to 10000, useInt has ~5x increase in
run time. And when input doubles again to 20000 theres is another ~5 increase in 
run time. When we test with 30000 digits there is a ~3 increase in time vs 20000. 

This makes sense because I implemented an insertion sort. When I get a new digit 
I have to scan through the entire linkedlist to find its position, meaning as a 
list gets bigger linearly, there is a more than linear increase in run time. In a 
worst case scenario my code has to scan through n digits n times to sort an 
entire list. This makes it O(n^2) complexity, which makes sense because my
runtime is rising at a more than linear rate. If you graphed the times it'd 
probably be a quadratic.

NOTE I implemented a modified insertion sort which checks if a number was the 
smallest in the list, as well as if it was the largest, this means that instead of
a reverse sorted list being a worst case scenario, it is effectively the same as a
best case scenario (sorted list). 

These trends are not as visible in sort -n because sort was written with black
magic and is crazy fast. It's also probably not using insertion sort for large 
inputs, probably quick sort or some other complex sorting algo which is faster
than O(n^2).

Testing
------------------------
//commands i use to test each of my number files in a directory 
for i in *; do time ../useIntList < $i > /dev/null; echo $i; done
for i in *; do time sort -n < $i > /dev/null; echo $i; done

//commands i used to generate files
n=30000; seq $n > 2alreadySortedNoDuplicates;seq $n | sort -nr > 3reverseSortedNoDuplicates;seq $n | sort -R > 1randomOrderNoDuplicates;../randList $n | sort -n > 5alreadySortedDups;../randList $n | sort -nr > 6reverseSortedDups;../randList $n > 4randomOrderDups

//command I used to generate results 
//note this command needs to be run in the nums folder
//grep will get the user time, sed grabs the character between m and s and 
removes the period, awk averages all the times then prints the avg in 2 sigfigs 
in scientific notation, xarg presents the pipe as a paramater to printf which 
formats it as a floating point of length 5 with leading 0s and no digits after 
the decimal point

//useIntList
for i in *; do echo $i; for ((n=0;n<5;n++)) do { time ../useIntList < $i > /dev/null; } done |& grep user | sed -e 's/m\(.*\)s/\1/' -e 's/\.//g' | awk '{ total += $2; count++ } END { printf "%.2g", total/count }' | xargs printf "%05.f\n"; done

//sort -n
for i in *; do for ((n=0;n<5;n++)) do { time sort -n < $i > /dev/null; } done |& grep user | sed -e 's/m\(.*\)s/\1/' -e 's/\.//g' | awk '{ total += $2; count++ } END { printf "%.2g", total/count }' | xargs printf "%05.f\n"; done
